# Battleship_Modeling
## Premise
The original version of this project simply collected data on a ~60,000 battleship games and then I used Apple's CreateML to train a model. Unfortunantely I had not realized their was a bug in the ship placement causing ships to only be placed vertically. After learning alot about tensorflow and the many aspects of Machine Learning I took another stab at the project.
## The Model
### Input Pipeline
The board is split into 6 channels and a 10 by 10 grid (6,10,10). Values in channel 0 are 0.0 or 1.0. 1 represents a square that has been shot at and was a miss. Values in channels 1 through 5 correspond to each of the 5 ships and values are either -1.0, 0.0 or 1.0. -1.0 represents a ships has been sunk. 1.0 represents a hit but not a sunk ship. 
### Layers
The model uses convolutions to intially identify patterns in the data. The convolutions are then concatenated and passed through a 2d (fully connected) locally connected layer with sigmoid activation and then flattened and passed to output. The model is expected to have high confidence in multiple squares and is trained with multi-labels thus sigmoid is used over softmax. I use a locally connected layer here because it was *slightly* faster when using Accelerated Linear Algebra XLA. I chose to use 'selu' activation instead of 'relu' to prevent some weights from dying at the expense of longer training times. I also experimented with using L1 and L2 constraints. L1 is used in the convolutions because the data in some adjacent squares may not have an effect on prediction. L2 is used is used in the LC layer because all convolutions should have an effect on each each prediction however the more important a filter the greater of an impact it should have thus regularization is used to prevent overfitting.
## The Battleship Enviroment
Starting with the first iteration of the project I had set out to use Open AI Gym. It may not be the absolute fastest but it is most certainly up there. It is also a module which has been designed for reinforcment learning in the same setting I am currently engaged in. Over the developmment of the project I have tested a number of different methods to speed up the enviroment and it ultimately takes <0.8 micro secconds to setup and play a single game. Numpy and copys are used wherever is applicable to squeeze every nano seccond out of the enviroment. 
### Setting Up The Enviroment
Firstly the observvation and action spaces of the enviroment are defined. Next the enviroment is seeded and some class variables are defined
### Reseting The Enviroment
Before any space can be shot at first the enviroment must be reset. First the enviroment starts by placing the ships. A random space and direction are chosen and then the space is tested to see if there are any conflicts. If there are not any the ship is placed and then placements are attempted for the next ship in the list. Once all ships are placed the observation tracker, hidState, is opened up. Lastly the omniscent multi-label tracker is setup and class variables are reset.
### Playing the game
Shot keeping is quite simple. If a slot is empty flip it to a miss, if a slot contains a ship flip it to a hit, if it a ship is sunk flip all slots for that ship to sunk. Most importantly of all is if a slot is shot at twice by the model that is considered an immediate loss and the game is stopped. At the end of the move the observation, expected action, whether a hit was scored and whether the game is over is all returned to the model.
## Training
A set number of games are played. For each move in each game either a random space is chosen or a prediction is obtained from the model dependent on the Epsilon. The observation and expected prediction is recorded for each of the values. These records are then shuffled, and the model is trained on batches of the data. Finally diagnostics are printed pertaining to the previous games. 
